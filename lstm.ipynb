{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from collections import deque\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293603 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "c stzcelas tbwzyfs sqdodet mqvrful webdeo ezstpjadwiemeuiuvlwyogxvfenet wo mu gg\n",
      "msii wicwaxce mcmt saoeusbn sdvjxcrkstitzeen irjiaeinclnpymr goabjomrhidmtc aacd\n",
      "leeficqg yna  uottre gp w eoebmkssrpvw  uswpsarw  ctkpvop mmikn rxqfit odia dl g\n",
      "vmiidkcdm irtyooe fs bcrcdjur  u e  o xbugbspmhji nlhjl atmjwtqcqmqncn  eti xiy \n",
      "uc rbkbhzwgnhmndb pamv c  gx gfqumev jhvh viwdhtfkhcm mmssnnekbt rlnt hhtzn b ep\n",
      "================================================================================\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 1000: 2.011769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "x be fice bluseckuen in sectreev seve who scames a the sebly in the new ablemof \n",
      "jer resedrency enod an of one nine eight es one nine seven five sixher of specer\n",
      "que will be augrays of cangival night freesory weke of the fure refermantily in \n",
      "y it is elap and five is cuarmice whire seven courclasion in sopf ani is foon si\n",
      "phrote whil and perning copple brecin twres be sabe connuterialion beatho x inct\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 2000: 1.719327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "x one six six five seven putling the maningsy javinament of buir longeral of is \n",
      "j ground oftensian diget is do kidk durayles and s known de necinist production \n",
      "la we gasely thoughtin basper lenalime be adisterianity pridical werponk formati\n",
      "qu yuirped the chopating inentiam its and in dif diallowing englise naventally i\n",
      "wn has ninitics compriansmy the not de ngeders lowg phowing dunce the wessed yev\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3000: 1.658443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "y backs playe was stanq eurds the of passackaic stanve with syven infiliom drigi\n",
      "gropabinms and language one nine two othee pater imialdic ofvers as the shigh no\n",
      "urg swawled he hojorge inceesid corto quencods of gaces counit the cheist indivi\n",
      "bligh official for which ins the film st revideraliodly a transled fat both scho\n",
      "y maintouss with will reland e mat a mostsola primon reased to wa found the inst\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.646915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "ticians lahgras rupp quices a prescanary secondon excounted after chesk of hole \n",
      "za publications of have therez or casing the days as there henlos his lears conv\n",
      "y blat refers ubzer to mammedpusted begins the geoper the inculistists scondhild\n",
      "w wotle afthering a grove progropser side briams slovel histo kide texple out th\n",
      "qually bull however is in herist jay of qull proves armporess two zero zero zero\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5000: 1.619877 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "pergy cloreming is cameny push smmand gerach piement wht news a zerv of one nine\n",
      "y of that geronocian yemet one nine six the sumericy rumagur cones by politima a\n",
      "ent or west ire is the her prosen has deverowe drabughey mugaally as elles of th\n",
      "jopticel cooperated is ratwelle mount over mun entcosie in one zero zero zero an\n",
      "a vamufly four a callit one eight zero zero intincountent and some officially fo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6000: 1.576063 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "phardy and century air is relater was is had long of their onie youghing of has \n",
      "eled tock base beatrea nine saze national been joinning wyster morded from on ca\n",
      "le on the that though impemony not church also never a proclamens minet from int\n",
      "jetals they is the west of indepictiy playter instratifivity composive that sask\n",
      "ided become if his jother estua was speciement a cacement are the originition ba\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.565775 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "n from colosect authority japo his which particulted sombals and designe is avar\n",
      "buctray commught as a mairia the polion fiftermation is steetal limited the was \n",
      "jorey differentation of the were aballuropory sugic establish endix seenadnt sya\n",
      "st skletalds information foitho guais and despitute otherspoted p of and shysiva\n",
      "tan ciage a not would builian pebariticated of helving coinmy chutline above the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      #if step % (summary_frequency * 10) == 0:\n",
    "      # Generate some samples.\n",
    "      print('=' * 80)\n",
    "      for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = characters(feed)[0]\n",
    "        reset_sample_state.run()\n",
    "        for _ in range(79):\n",
    "          prediction = sample_prediction.eval({sample_input: feed})\n",
    "          feed = sample(prediction)\n",
    "          sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "      print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each gate has an input, a previous output (memory), and a bias. There are four gates, each of which, in the definition of the LSTM cell, is involved in a matrix multiplication. Thus, four matrix multiplications in the definition of the LSTM cell involve the input, four involve the output, and four involve the bias.\n",
    "\n",
    "First, define variables that are four times larger. The input will be called `input_matrix`, the output will be called `output_matrix`, and the bias will be called `bias_matrix`. Their definitions are as in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  input_matrix = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  output_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bias_matrix = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    total_product = tf.matmul(i, input_matrix) + tf.matmul(o, output_matrix) + bias_matrix\n",
    "    inputs, forgets, memory, output = tf.split(total_product, 4, axis=1)\n",
    "    \n",
    "    input_gate = tf.sigmoid(inputs)\n",
    "    forget_gate = tf.sigmoid(forgets)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(memory)\n",
    "    output_gate = tf.sigmoid(output)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "    \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298196 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "kjnemuuh  kiwrh ozlihfl  ktoiihtajz h btbicomkm  lkaylks nvgihonepqyemjde grsluq\n",
      "os i ry or lmxg wlqqyzitemhfe sfelytnxaqtoixs z igoer twlgspcajfvb vckbynh qebne\n",
      "yocreeirnl bdtlsh nl o is knlwsxcnjkddpkkrsulr eu hju c bmdedq w  p igsxeo lnh e\n",
      " gvr sri  fbd zyeouobantnfjwxc  oavddueilwaercdhtzgjmnp lqn ci eeobhlftqthtasqh \n",
      "nbgpi yyxjhgkcze p ggyt  ig s ihe hyrvhr ezehjlvbbteeanxaypvnarieshtpf l pvtuiwh\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 1000: 2.028540 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "================================================================================\n",
      " oppect stain c ofth pmirecture igsais v morent stort timod of alreist the gerso\n",
      "mogr zork not madimption setsiem the ised the orych pile netwoul of lives an bou\n",
      "ron parternts oid stancr hy methover crost astr x for requmporz the undic rollyy\n",
      "quased aulic are parts dedu scains tfolsy y the expers his lut ussive brith thre\n",
      "que tradic ompuctry in evering precluge mara distic lysiands he pabirece tomatho\n",
      "================================================================================\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 2000: 1.737001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "war ank the poyed perriquary bloqih in the geilally atstons that is such vollyga\n",
      "y brise nony technies verm is intinuer for b one not catemblan uses s preance re\n",
      "fentagus ameic englall it is the chimcnolital denatter of spectly thick and of d\n",
      "a babill pupilenal leven sont the grovers for bat one eight innovernial softer c\n",
      "fages worlaciate re actuated two by seid this a kaninuer chered six sport while \n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3000: 1.687957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      " the laymene one more remolated chisial put exteltard the ades temprandony to th\n",
      "st non the to applitation seedel rusinely that is a libit degith and atisminn of\n",
      "ation of desendum a a date not callicator and one enading the pub kt action of t\n",
      "th of gener is trpopup unverrou and penturistary to miniffiearol of firsble twan\n",
      "cherf refaantem withandom to xumpled allovished sactims eoved storiated jrince t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4000: 1.650941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "werch from cooside pole id deque assike and relused force this nes the deplane m\n",
      "on smems words and the greopere stann impeousts frenchimple to was a leffargua l\n",
      "gial marchiveler one eight metile cross issust foundarate to nezour cloads wargs\n",
      "vely deal counder agending marroussas nekrokic m fumcorsual indian exisaty the o\n",
      "ins convivinge bardedy suppets which the cite fall videt of read equar of shotto\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5000: 1.619803 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "nail gussuance well forumete as constrop neer the form though the willity places\n",
      "mes to larging apagebs in the frox the cointul instrumation neiurd in the rather\n",
      "jantace can tunnel trowed of difyrea in the all sccupsia tione it the eiloria co\n",
      "umest tites all letsional i from mos to be island some bands gueraces for earity\n",
      "dual the phare subfand oi chardee in mosing tows in furicu there alsommered whoo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6000: 1.586196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "ganga fooldevers condamagy and hirsope the yen balder alber sended byhnig in the\n",
      "vened and formed and world andy followled these at minnistroues a calpomb region\n",
      "k as well as charas and for the mean ware more world by f estackh and almore ste\n",
      "bue revelsic liagraturat in merrar introductive carcuda hilkhraniror levone avid\n",
      "rice eight one nine seven five the til of a cavila figuric spiritle glogisns war\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7000: 1.577970 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "war to paty greewe workar an in the broumally roberinaty is styl commution natio\n",
      "man himseracter missian briz xlonia is for the nouran the first a cons one pribi\n",
      "or of a seen six upers hafe about and the catching by zero one eight seven one o\n",
      "isman the according the span criqulers usey stwe shows all his krong elects as l\n",
      "rion at cator prodided and scientisties and nate manzengure to retorded pripon o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      #if step % (summary_frequency * 10) == 0:\n",
    "      # Generate some samples.\n",
    "      print('=' * 80)\n",
    "      for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = characters(feed)[0]\n",
    "        reset_sample_state.run()\n",
    "        for _ in range(79):\n",
    "          prediction = sample_prediction.eval({sample_input: feed})\n",
    "          feed = sample(prediction)\n",
    "          sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "      print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $vocabulary\\_size^2$ possible bigrams, as each is a combination of a letter or whitespace and with a letter or whitespace. Since the value will be reused, it will be stored in the variable `num_bigrams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_bigrams = vocabulary_size * vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch generation function currently generates inputs in the form of 1-hot encodings. Since we don't want to feed 1-hot encodings to the LSTM, we can start by modifying the function to generate batches of single number encodings of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each training batch is a length 11 list of length 64 lists of integers.\n",
      " Each integer represents a bigram.\n",
      "Each validation batch is a length 2 list of length 1 lists of integers.\n",
      " Each integer represents a bigram.\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class int_BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = list()\n",
    "    for b in range(self._batch_size):\n",
    "        val = char2id(self._text[self._cursor[b]]) * vocabulary_size\n",
    "        if self._cursor[b]+1 > self._text_size:\n",
    "            val += char2id(' ')\n",
    "        else:\n",
    "            val += char2id(self._text[self._cursor[b]])\n",
    "        batch.append(val)\n",
    "        self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "p2_train_batches = int_BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "p2_valid_batches = int_BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "ex_batch = p2_train_batches.next()\n",
    "print('Each training batch is a length {} list of length {} lists of integers.\\n Each integer represents a bigram.'.format(len(ex_batch), len(ex_batch[0])))\n",
    "\n",
    "ex_batch = p2_valid_batches.next()\n",
    "print('Each validation batch is a length {} list of length {} lists of integers.\\n Each integer represents a bigram.'.format(len(ex_batch), len(ex_batch[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the generate function generates batches of integers rather than one hot vectors, we also must modify `logprob()` to accept integers rather than one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2_logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  one_hot_labels = np.zeros(predictions.shape)\n",
    "  for r, c in enumerate(labels):\n",
    "    one_hot_labels[r,c] = 1.0\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(one_hot_labels, -np.log(predictions))) / one_hot_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that accepts a vector of probabilities of bigrams and returns the most probable character representation. The first characters are derived from the integer representations by floor division and the second by modulus, in accordance with the definition in _next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probable_bigram(probabilities):\n",
    "    return ['{0} {1}'.format(id2char(c // vocabulary_size), id2char(c % vocabulary_size)) for c in np.argmax(probabilities, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also modify the other given helper functions accordingly:\n",
    "* `sample()` to generate vectors of dimension 1 by num_bigrams.\n",
    "* `random_distribution()` to generate vectors of dimension 1 by num_bigrams.\n",
    "* `characters()` to modify id values according to the increased vocabulary size (floor division is appropriate because only the first member of the bigram is needed).\n",
    "* `batches2string()` to accept batches composed of single integers rather than one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p2_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, num_bigrams], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def p2_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, num_bigrams])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def p2_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c // vocabulary_size)\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def p2_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, probable_bigram(b))]\n",
    "  return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Regularization\n",
    "Now, we can simply modify the model to accept bigram embeddings as created in the function of the above class. We also introduce the model to the concept of dropout.\n",
    "\n",
    "Srivastava (2013) argued that dropout does not work well with RNNs and, as a result, overfitting is a problem with large RNNs that don't use regularization. Bayer, et. al (2013) argue that this is because recurrence amplifies noise, which intereferes with learning. However, Zaremba, et. al show that the issue can be remedied by applying the dropout operator only to non-recurrent connections. The argument is that one does not want to erase all information from events that occurred in the past, as this makes it difficult for the LSTM to learn to store information for long periods of time.\n",
    "\n",
    "The following is a bigram-based LSTM with dropout introduced, using this principle and applying dropout only during training and on only on the inputs (the memory (e.g., in states) and outputs are clearly involved in recurrent connections). Further, dropout is only applied during training, in accordance with previous teachings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  input_matrix = tf.Variable(tf.truncated_normal([num_bigrams, 4*num_nodes], -0.1, 0.1))\n",
    "  output_matrix = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bias_matrix = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, num_bigrams], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([num_bigrams]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, train=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_values = tf.nn.embedding_lookup(input_matrix, i)\n",
    "    \n",
    "    # We only introduce dropout to the values not directly involved in recurrent connections\n",
    "    if train:\n",
    "      input_values = tf.nn.dropout(input_values, keep_prob)\n",
    "    \n",
    "    total_product = input_values + tf.matmul(o, output_matrix) + bias_matrix\n",
    "    inputs, forgets, memory, output = tf.split(total_product, 4, axis=1)\n",
    "    \n",
    "    input_gate = tf.sigmoid(inputs)\n",
    "    forget_gate = tf.sigmoid(forgets)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(memory)\n",
    "    output_gate = tf.sigmoid(output)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "    \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      # Account for the fact that the inputs are of no length, as they are single integers in the range [1, num_bigrams]\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                                 #   and each datum represents a bigram\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, True)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  # Again, account for the fact that the input are single integers, rather than one-hot vectors\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591968 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.21\n",
      "Validation set perplexity: 327.08\n",
      "Average loss at step 1000: 2.354676 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.04\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 2000: 2.045547 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.80\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3000: 1.991544 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 4000: 1.959655 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 5000: 1.936325 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 6000: 1.897294 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 7000: 1.883300 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.03\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = p2_train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(p2_logprob(predictions, labels))))\n",
    "#       if step % (summary_frequency * 10) == 0:\n",
    "#       # Generate some samples.\n",
    "#       print('=' * 80)\n",
    "#       for _ in range(5):\n",
    "#         feed = p2_sample(p2_random_distribution())\n",
    "#         sentence = p2_characters(feed)[0]\n",
    "#         feed = [np.argmax(feed)]\n",
    "#         reset_sample_state.run()\n",
    "#         for _ in range(79):\n",
    "#           feed = [np.argmax(feed)]\n",
    "#           prediction = sample_prediction.eval({sample_input: feed})\n",
    "#           feed = p2_sample(prediction)\n",
    "#           sentence += p2_characters(feed)[0]\n",
    "#       print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = p2_valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + p2_logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\n",
    "This resource was used: https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/1-seq2seq.ipynb\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 2\n",
    "EOS = 27\n",
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "input_embedding_size = 20 #len of what is fed to the model\n",
    "## feed the model char2id() vals (e.g., in [1,27])\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "#           diff in size of encoder and decoder hidden units b/c want output val to be a bit diff from input\n",
    "\n",
    "#input placehodlers\n",
    "encoder_inputs = tf.placeholder(shape=(num_unrollings+1, batch_size), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_inputs = tf.placeholder(shape=(num_unrollings+2, batch_size), dtype=tf.int32, name='decoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(num_unrollings+2, batch_size), dtype=tf.int32, name='decoder_targets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 27 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return vocabulary_size - 1\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0 and dictid < vocabulary_size:\n",
    "    if dictid == 27:\n",
    "        return ' '\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def sentence(ints):\n",
    "    sent = ''\n",
    "    for num in ints:\n",
    "        sent += id2char(num)\n",
    "    return sent\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embeddings\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define encoder, decoder\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "encoder_cell, encoder_inputs_embedded, dtype=tf.float32, time_major=True\n",
    ")\n",
    "\n",
    "del encoder_outputs\n",
    "\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "decoder_cell, decoder_inputs_embedded, initial_state=encoder_final_state,\n",
    "dtype=tf.float32, time_major=True, scope=\"plain_decoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocabulary_size)\n",
    "predict = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocabulary_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator_seq2seq(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "#   def next(self):\n",
    "#     \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "#     batch = np.zeros(shape=(self._batch_size, 20), dtype=np.int32)\n",
    "#     for index in range(batch_size):\n",
    "#       for b in range(20):\n",
    "#         batch[b, index] = char2id(self._text[self._cursor[index]])\n",
    "#         self._cursor[index] = (self._cursor[index] + 1) % self._text_size\n",
    "#     return batch\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, 1), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, [id2char(c) for c in b])]\n",
    "  return s\n",
    "\n",
    "batches = BatchGenerator_seq2seq(text, batch_size, num_unrollings)\n",
    "\n",
    "batch = batches.next()\n",
    "\n",
    "# print('one encoder input: {}'.format(batches2string(encoder_inputs)[0]))\n",
    "# print('one decoder target: {}'.format(batches2string(decoder_targets)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "  encoder_inputs_ = np.copy(batches.next())[:,:,0]\n",
    "  \"\"\"\n",
    "  decoder_inputs_ = np.copy(encoder_inputs_)\n",
    "  decoder_inputs_ = np.insert(decoder_inputs_, 0, EOS, axis = 1)\n",
    "  \"\"\"\n",
    "#   decoder_inputs_ = np.insert(decoder_inputs_, 0, EOS, axis = 1)\n",
    "  decoder_targets_ = encoder_inputs_[list(reversed(range(11))),:]\n",
    "  decoder_targets_ = np.append(decoder_targets_, np.zeros(shape=(1,64), dtype=np.int32) + EOS, axis=0)\n",
    "#   decoder_targets_ = [np.append(sequence,-1) for sequence in decoder_targets]\n",
    "\n",
    "  decoder_inputs_ = np.copy(encoder_inputs_)\n",
    "  decoder_inputs_ = np.insert(decoder_inputs_, 0, EOS, axis = 0)\n",
    "#   print(decoder_inputs_.T)\n",
    "#   print(encoder_inputs_.shape, '\\n\\n', decoder_inputs_.shape, '\\n\\n', decoder_targets_.shape)\n",
    "#   return encoder_inputs_, decoder_inputs_, decoder_targets_\n",
    "  return {\n",
    "      encoder_inputs: encoder_inputs_,\n",
    "      decoder_inputs: decoder_inputs_,\n",
    "      decoder_targets: decoder_targets_\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 10000\n",
      "  minibatch loss: 0.08387967199087143\n",
      "  sample 1:\n",
      "    input:  o zero five \t\tprediction:  evif orez o \n",
      "  sample 2:\n",
      "    input:   the french \t\tprediction:  hcnerf eht  \n",
      "  sample 3:\n",
      "    input:  d a half in \t\tprediction:  ni flah a d \n",
      "\n",
      "step 20000\n",
      "  minibatch loss: 0.09345012903213501\n",
      "  sample 1:\n",
      "    input:  s used orig \t\tprediction:  giro desu s \n",
      "  sample 2:\n",
      "    input:   process th \t\tprediction:  ht ssecorp  \n",
      "  sample 3:\n",
      "    input:  lne near al \t\tprediction:  la raen enl \n",
      "\n",
      "step 30000\n",
      "  minibatch loss: 0.07273869961500168\n",
      "  sample 1:\n",
      "    input:  orical and  \t\tprediction:   dna laciro \n",
      "  sample 2:\n",
      "    input:  res copyrig \t\tprediction:  girypoc ser \n",
      "  sample 3:\n",
      "    input:  ke stegosau \t\tprediction:  uasogets ek \n",
      "\n",
      "step 40000\n",
      "  minibatch loss: 0.07310158759355545\n",
      "  sample 1:\n",
      "    input:  articipate  \t\tprediction:   etapiciora \n",
      "  sample 2:\n",
      "    input:  the french  \t\tprediction:   hcnerf eht \n",
      "  sample 3:\n",
      "    input:  te butane a \t\tprediction:  a enatub et \n",
      "\n",
      "step 50000\n",
      "  minibatch loss: 0.05544769763946533\n",
      "  sample 1:\n",
      "    input:   cards thus \t\tprediction:  suht sdrac  \n",
      "  sample 2:\n",
      "    input:  tion debate \t\tprediction:  etabed noit \n",
      "  sample 3:\n",
      "    input:  a otter pel \t\tprediction:  lep retto a \n",
      "\n",
      "step 60000\n",
      "  minibatch loss: 0.055725932121276855\n",
      "  sample 1:\n",
      "    input:  ical associ \t\tprediction:  icossa laci \n",
      "  sample 2:\n",
      "    input:  lth relatio \t\tprediction:  oitaler htl \n",
      "  sample 3:\n",
      "    input:   and gregor \t\tprediction:  rogerg dna  \n",
      "\n",
      "step 70000\n",
      "  minibatch loss: 0.042277608066797256\n",
      "  sample 1:\n",
      "    input:  e time who  \t\tprediction:   ohw emit e \n",
      "  sample 2:\n",
      "    input:   st rmer ad \t\tprediction:  da remr ts  \n",
      "  sample 3:\n",
      "    input:  any indigen \t\tprediction:  negidni yna \n",
      "\n",
      "step 80000\n",
      "  minibatch loss: 0.030902648344635963\n",
      "  sample 1:\n",
      "    input:  ic i theodo \t\tprediction:  oloeht i ci \n",
      "  sample 2:\n",
      "    input:  tems and th \t\tprediction:  ht dna smet \n",
      "  sample 3:\n",
      "    input:  n the leaf  \t\tprediction:   fael eht n \n",
      "\n",
      "step 90000\n",
      "  minibatch loss: 0.02827599085867405\n",
      "  sample 1:\n",
      "    input:  n a creativ \t\tprediction:  vitaerc a n \n",
      "  sample 2:\n",
      "    input:  val one st  \t\tprediction:   ts eno lav \n",
      "  sample 3:\n",
      "    input:   saturn v l \t\tprediction:  l v nrutas  \n",
      "\n",
      "step 100000\n",
      "  minibatch loss: 0.04048926755785942\n",
      "  sample 1:\n",
      "    input:  s further t \t\tprediction:  t rertruf s \n",
      "  sample 2:\n",
      "    input:  tinctive he \t\tprediction:  eh evitcnit \n",
      "  sample 3:\n",
      "    input:  storically  \t\tprediction:   yllacirots \n"
     ]
    }
   ],
   "source": [
    "loss_vals = []\n",
    "\n",
    "num_steps = 100001\n",
    "summary_frequency = 10000\n",
    "\n",
    "for step in range(num_steps):\n",
    "  fd = next_feed()\n",
    "  _, l = sess.run([train_op, loss], fd)\n",
    "  loss_vals.append(l)\n",
    "\n",
    "  if step % summary_frequency == 0 and step:\n",
    "    print('\\nstep {}'.format(step))\n",
    "    print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "    predict_ = sess.run(predict, fd)\n",
    "    for i in range(3):\n",
    "      print('  sample {}:'.format(i + 1))\n",
    "      inp = np.transpose(fd[encoder_inputs])[i]\n",
    "      pred  = np.transpose(predict_)[i]\n",
    "      print('    input: ', sentence(inp), '\\t\\tprediction: ', sentence(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model takes several thousand steps, around 4000-5000 steps) to start performing the task well, though it still has trouble with longer words after much training. Obviously, the loss values become quite diminutive, but the model still does not perform perfectly and thus does not seem well suited to the task."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
